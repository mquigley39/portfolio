{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Classification using a Convolutional Neural Network and various architectures\n",
    "## Madeline Quigley\n",
    "## 2023.04.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os, errno\n",
    "import cv2 as cv\n",
    "from tensorflow import keras\n",
    "\n",
    "file = r\"C:\\Users\\Madeline\\Documents\\Data Mining\\Python\\Grocery\\Weeds\\Weed-4class-34\\*.jpg\"\n",
    "glob.glob(file)\n",
    "\n",
    "images = [cv.imread(image) for image in glob.glob(file)]\n",
    "type(images)\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 52  60  59]\n",
      "   [ 49  56  55]\n",
      "   [ 59  64  62]\n",
      "   ...\n",
      "   [ 81  79  75]\n",
      "   [ 55  54  50]\n",
      "   [ 38  37  33]]\n",
      "\n",
      "  [[ 58  66  65]\n",
      "   [ 61  68  67]\n",
      "   [ 51  56  55]\n",
      "   ...\n",
      "   [ 60  59  55]\n",
      "   [ 58  57  53]\n",
      "   [ 47  46  42]]\n",
      "\n",
      "  [[ 54  62  61]\n",
      "   [ 59  66  65]\n",
      "   [ 49  54  52]\n",
      "   ...\n",
      "   [ 55  54  50]\n",
      "   [ 53  52  48]\n",
      "   [ 40  39  35]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[225 212 196]\n",
      "   [154 144 131]\n",
      "   [186 180 174]\n",
      "   ...\n",
      "   [102  91  85]\n",
      "   [ 81  69  61]\n",
      "   [ 46  32  25]]\n",
      "\n",
      "  [[247 233 214]\n",
      "   [174 162 145]\n",
      "   [187 180 170]\n",
      "   ...\n",
      "   [ 69  58  51]\n",
      "   [ 87  74  66]\n",
      "   [ 45  30  23]]\n",
      "\n",
      "  [[252 237 217]\n",
      "   [199 186 168]\n",
      "   [193 184 172]\n",
      "   ...\n",
      "   [ 41  30  24]\n",
      "   [ 74  59  52]\n",
      "   [ 54  38  31]]]\n",
      "\n",
      "\n",
      " [[[ 46  55  59]\n",
      "   [ 54  66  60]\n",
      "   [ 47  64  41]\n",
      "   ...\n",
      "   [111 102  93]\n",
      "   [116 106  97]\n",
      "   [112 101  93]]\n",
      "\n",
      "  [[ 51  57  61]\n",
      "   [ 49  61  54]\n",
      "   [ 58  74  51]\n",
      "   ...\n",
      "   [ 89  80  71]\n",
      "   [ 88  78  70]\n",
      "   [ 77  66  58]]\n",
      "\n",
      "  [[ 58  62  66]\n",
      "   [ 46  55  49]\n",
      "   [ 53  66  45]\n",
      "   ...\n",
      "   [ 85  76  67]\n",
      "   [ 86  75  67]\n",
      "   [ 76  65  57]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 80  68  62]\n",
      "   [ 73  61  55]\n",
      "   [ 62  50  44]\n",
      "   ...\n",
      "   [ 49  44  41]\n",
      "   [ 54  49  46]\n",
      "   [ 72  67  64]]\n",
      "\n",
      "  [[ 78  66  60]\n",
      "   [ 74  62  56]\n",
      "   [ 71  58  51]\n",
      "   ...\n",
      "   [ 41  36  33]\n",
      "   [ 45  40  37]\n",
      "   [ 34  30  26]]\n",
      "\n",
      "  [[ 65  53  47]\n",
      "   [ 89  77  71]\n",
      "   [ 88  74  68]\n",
      "   ...\n",
      "   [ 32  27  24]\n",
      "   [ 49  44  41]\n",
      "   [ 22  17  14]]]\n",
      "\n",
      "\n",
      " [[[101  85  78]\n",
      "   [130 114 106]\n",
      "   [130 113 104]\n",
      "   ...\n",
      "   [ 81  84  83]\n",
      "   [ 92  95  98]\n",
      "   [230 235 238]]\n",
      "\n",
      "  [[107  91  84]\n",
      "   [123 107  99]\n",
      "   [126 109 100]\n",
      "   ...\n",
      "   [ 55  59  57]\n",
      "   [ 57  61  64]\n",
      "   [236 241 241]]\n",
      "\n",
      "  [[100  85  76]\n",
      "   [102  87  78]\n",
      "   [107  90  81]\n",
      "   ...\n",
      "   [ 52  54  53]\n",
      "   [ 45  48  50]\n",
      "   [233 238 239]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[251 179 144]\n",
      "   [255 190 153]\n",
      "   [253 195 153]\n",
      "   ...\n",
      "   [ 46  58  59]\n",
      "   [ 50  66  68]\n",
      "   [116 134 137]]\n",
      "\n",
      "  [[250 159 130]\n",
      "   [251 177 143]\n",
      "   [237 178 137]\n",
      "   ...\n",
      "   [ 51  64  65]\n",
      "   [ 40  57  60]\n",
      "   [ 84 105 107]]\n",
      "\n",
      "  [[235 133 106]\n",
      "   [249 167 135]\n",
      "   [225 168 127]\n",
      "   ...\n",
      "   [ 56  69  71]\n",
      "   [ 47  65  68]\n",
      "   [ 66  87  89]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[127 156 102]\n",
      "   [131 158 107]\n",
      "   [127 151 110]\n",
      "   ...\n",
      "   [ 77  91  66]\n",
      "   [ 50  65  37]\n",
      "   [ 45  60  29]]\n",
      "\n",
      "  [[115 143  91]\n",
      "   [128 154 106]\n",
      "   [115 140  97]\n",
      "   ...\n",
      "   [ 92 104  82]\n",
      "   [ 91 103  79]\n",
      "   [150 163 136]]\n",
      "\n",
      "  [[120 146  98]\n",
      "   [128 154 108]\n",
      "   [ 96 121  78]\n",
      "   ...\n",
      "   [111 119 102]\n",
      "   [ 88  96  77]\n",
      "   [ 50  59  38]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 48  53  33]\n",
      "   [ 45  50  34]\n",
      "   [ 71  74  69]\n",
      "   ...\n",
      "   [108 143 101]\n",
      "   [ 54  80  43]\n",
      "   [ 58  79  44]]\n",
      "\n",
      "  [[ 40  45  20]\n",
      "   [ 33  37  19]\n",
      "   [ 62  63  56]\n",
      "   ...\n",
      "   [111 146 103]\n",
      "   [ 65  91  52]\n",
      "   [ 83 101  67]]\n",
      "\n",
      "  [[ 53  57  29]\n",
      "   [ 59  62  43]\n",
      "   [ 63  64  57]\n",
      "   ...\n",
      "   [ 99 135  92]\n",
      "   [ 66  91  52]\n",
      "   [ 84 101  67]]]\n",
      "\n",
      "\n",
      " [[[113 122  72]\n",
      "   [160 169 117]\n",
      "   [174 184 125]\n",
      "   ...\n",
      "   [255 255 253]\n",
      "   [255 253 255]\n",
      "   [255 252 255]]\n",
      "\n",
      "  [[114 123  73]\n",
      "   [162 171 119]\n",
      "   [177 187 128]\n",
      "   ...\n",
      "   [255 255 253]\n",
      "   [255 253 255]\n",
      "   [255 252 255]]\n",
      "\n",
      "  [[114 123  73]\n",
      "   [164 173 120]\n",
      "   [179 189 130]\n",
      "   ...\n",
      "   [255 255 254]\n",
      "   [255 253 255]\n",
      "   [255 251 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 58  53  55]\n",
      "   [ 30  27  30]\n",
      "   [ 46  45  45]\n",
      "   ...\n",
      "   [110 113 104]\n",
      "   [166 171 165]\n",
      "   [ 69  74  70]]\n",
      "\n",
      "  [[ 44  39  41]\n",
      "   [ 27  24  26]\n",
      "   [ 48  48  48]\n",
      "   ...\n",
      "   [110 114 105]\n",
      "   [ 80  86  80]\n",
      "   [115 121 116]]\n",
      "\n",
      "  [[ 30  26  28]\n",
      "   [ 22  19  22]\n",
      "   [ 46  45  45]\n",
      "   ...\n",
      "   [129 133 124]\n",
      "   [ 83  89  83]\n",
      "   [126 133 128]]]\n",
      "\n",
      "\n",
      " [[[112 116  97]\n",
      "   [101 107  91]\n",
      "   [ 92 103  91]\n",
      "   ...\n",
      "   [110 108  99]\n",
      "   [107 103  92]\n",
      "   [ 79  73  60]]\n",
      "\n",
      "  [[ 89  95  70]\n",
      "   [ 97 105  83]\n",
      "   [130 140 124]\n",
      "   ...\n",
      "   [140 135 128]\n",
      "   [134 127 116]\n",
      "   [111 102  89]]\n",
      "\n",
      "  [[128 136 101]\n",
      "   [123 133 101]\n",
      "   [118 131 106]\n",
      "   ...\n",
      "   [142 133 126]\n",
      "   [131 119 110]\n",
      "   [120 108  96]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 93  91 103]\n",
      "   [100  99 111]\n",
      "   [ 92  94 105]\n",
      "   ...\n",
      "   [106 106 108]\n",
      "   [127 127 129]\n",
      "   [126 127 128]]\n",
      "\n",
      "  [[ 96  93 105]\n",
      "   [ 90  88 100]\n",
      "   [ 86  85  97]\n",
      "   ...\n",
      "   [143 146 147]\n",
      "   [130 134 135]\n",
      "   [112 115 116]]\n",
      "\n",
      "  [[106 102 114]\n",
      "   [100  97 109]\n",
      "   [ 89  87 100]\n",
      "   ...\n",
      "   [166 171 172]\n",
      "   [158 162 163]\n",
      "   [108 113 114]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = []\n",
    "for filename in glob.glob(file):\n",
    "    img = cv.imread(filename)\n",
    "    if img is not None:\n",
    "     # Resize image\n",
    "        img = cv.resize(img, (224, 224))\n",
    "        # Append to list\n",
    "        X.append(img)\n",
    "    else:\n",
    "        print(f\"Could not read image: {file}\")\n",
    "trainingSet = np.array(X)\n",
    "print(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinee apple' 'Chinee apple' 'Chinee apple' ... 'Rubber vine'\n",
      " 'Rubber vine' 'Rubber vine']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Reading Data\n",
    "df = pd.read_csv(r\"C:\\Users\\Madeline\\Documents\\Data Mining\\Python\\Grocery\\Weeds\\Weed-4class-34\\Weed-4class-34-labels.csv\")\n",
    "file_pattern = r\"C:\\Users\\Madeline\\Documents\\Data Mining\\Python\\Grocery\\Weeds\\Weed-4class-34\\*.jpg\"\n",
    "\n",
    "label_list = []\n",
    "\n",
    "for filename, label in zip(glob.glob(file), df[\"Species\"].tolist()):\n",
    "    im = cv2.imread(filename)\n",
    "    # append to list of labels\n",
    "    label_list.append(label)\n",
    "    \n",
    "classifications = np.array(label_list)\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2524, 224, 224, 3, 1)\n",
      "2524 train samples\n",
      "632 test samples\n",
      "y_train shape: (2524,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#loading data\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, classifications, test_size=.2)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = X_train.astype(\"float32\") / 255\n",
    "x_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train_encoded, num_classes=3)\n",
    "y_test = keras.utils.to_categorical(y_test_encoded, num_classes=3)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER WITH 8 3x3 FILTERS\n",
    "\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(224,224,3)),\n",
    "        keras.layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER WITH 8 3x3 FILTERS\n",
    "\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(224,224,3)),\n",
    "        keras.layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 132\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, val_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER WITH 8 5x5 FILTERS\n",
    "\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(224,224,3)),\n",
    "        keras.layers.Conv2D(8, kernel_size=(5, 5), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 132\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, val_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER WITH 8 7x7 FILTERS\n",
    "\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(224,224,3)),\n",
    "        keras.layers.Conv2D(8, kernel_size=(7, 7), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 132\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, val_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "# Source: https://keras.io/examples/vision/mnist_convnet/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
